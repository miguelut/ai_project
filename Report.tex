\documentclass[a4paper,11pt]{report}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[margin=1.0in]{geometry}
\usepackage{setspace}
\onehalfspacing

\title{AI: An Overview of Uninformed Searching and Applications}
\author{Dylan Thompson \and Michael M. Wright}

\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
This paper begins with a broad overview of uninformed search algorithms in the
field of Artificial Intelligence.  While search is useful for a variety of toy
problems such as the Wolf-Goat-Cabbage problem and the Missionaries and
Cannibals problem, this paper will discuss some of the characteristics of
problems that could potentially lead to use of uninformed searching in real
world scenarios.  The paper will then analyze some of the different search
algorithms as they relate to solving a graph search problem we created for
testing purposes.

\textbf{Don't forget to add your conclusions!}

\end{abstract}

\chapter{Uninformed Searching}
A small portion of solvable problems can be modeled in computers simply by
mapping the entire set of conditions to corresponding actions.  However, for
some problems this mapping is impractical if not impossible given the
constraints of our current technology.  In situations like these it becomes
imperative to build a \textbf{goal-based} agent.  Goal-based agents can, in
effect, consider future actions and evaluate the effect of those future
actions on reaching the desired solution.\cite{norvig}  The two main
subfields of goal-based agents are search and planning.\cite{wikiAgent} The
field of searching can be further whittled down into those algorithms that
utilize uninformed algorithms and those that utilize informed algorithms.
Later in the chapter we will look at the uninformed searching algorithms in
use today.  First, however, we must consider the criteria a problem must meet
before we can use a search agent to solve that problem.  It turns out that a
problem needs to be \textbf{well-defined} (or well-structured) prior to 
employing a searching agent to solve the problem.\cite{shun}

\section{Defining the Problem}
If we think about it there are some fairly obvious things we need to know
before we attempt to solve a problem.  For instance, we definitely need to be
able to check if we've actually solved it.  Also, we will presumably need a
place to start, or an initial state. Further, we need a set of actions our
agent can take along with a transition model that will return a successor state
to us after our agent has taken an action.  The final, less obvious thing we
need is a path cost.  \cite{norvig}  More concisely, a well-defined problem
consists of these five items:

\begin{itemize}
\item An \textbf{initial state}.
\item A set of \textbf{actions}.
\item A \textbf{transition model}.
\item A \textbf{goal-test}.
\item A \textbf{path cost}.\cite{norvig}
\end{itemize}

Once the problem is defined, we can go about building our agent to search for
a solution.  However, as we shall see in the next sections, not all solutions
were created equal.

\section{Search Properties}
In much of the literature regarding searching many authors refer to the
\textbf{search space}.  Somewhat informally, the search space is the solution
space defined by the set of all possible candidate solutions.\cite{wikiSpace}
More formally, the concept of \textbf{state space}, a synonym of search space,
has been defined as a 4-tuple containing the following elements:

\begin{itemize}
\item A set of states $N$.
\item A set of arcs $A$.
\item A nonempty subset of $N$, $S$, that contains start states.
\item A nonempty subset of $N$, $G$, that contains goal states.\cite{zhang99}
\end{itemize}

For goal-based agents it is generally easiest to picture the search space as a
tree, though it is more precisely a directed graph.\cite{zhang99}  We build the
tree by expanding the nodes.  Each node of the tree is a state, and the
connections between the nodes are abstractions of the relationship between
actions and the transition function.  In other words, each action will yield a
child node, and thus the tree expands for each action explored.  When we talk
about search we are really talking about different methods for expanding the
tree.\newline
\indent At any point in time prior to reaching a solution to our problem we
have a set of "temporary" leaf nodes (assuming at least one action is available
for the nodes) that can be expanded.  This is called the \textbf{frontier}
or open list.\cite{norvig}  The manner in which we choose which frontier node
to expand will be the defining characteristic of the searches we talk about
in the following sections.  Do we want to expand all the nodes at the same
depth first?  Or perhaps we want to follow a single path as deep as it will go
before moving on to the next path.  In addition, do we need to check for loops
in our path, or will the nature of our algorithm make checking for repeated
states a needless overhead?  Determining which state to expand next is generally
called the \textbf{search strategy}.\cite{norvig}\newline
\indent Implementing a search strategy requires some specific state that must
be tracked through each iteration of the algorithm. As mentioned before, the
entire state space is generally too large to keep in memory, and therefore the
state space is generally implicit in the search.\cite{norvig} At a minimum, the
search algorithm data structure needs the following four items:

\begin{itemize}
\item The \textbf{state}.
\item The node's \textbf{parent}.
\item The \textbf{action} that was applied to the parent.
\item The \textbf{path cost}.\cite{norvig}
\end{itemize}

Together with the elements of a well-defined problem it is easy to see how to
generate a child node given a parent node and an action.  Given a state and an
action we can apply the transition model to get the child node's state.  The
path cost is then computed with the information from the problem, and can be
constant or dynamic in nature.  Generally, choosing the data structure used in 
the search will have a huge determination on the search strategy.  A queue 
is FIFO and will have a very different search outcome from a LIFO stack or a 
priority queue.\cite{norvig}\newline

\section{Measuring Performance}
When searching we often find that some attributes of the search are more
important to us than others.  For instance, sometimes time may be of
paramount importance, while other times our search may operate under certain
memory constraints that make conservation of space take priority.  The various
algorithms we will consider in the next few sections will each have strengths
and weaknesses in the following categories:

\begin{itemize}
\item \textbf{Completeness}.
\item \textbf{Optimality}.
\item \textbf{Time Complexity}.
\item \textbf{Space Complexity}.
\end{itemize}

Completeness is a measure of whether an algorithm is guaranteed to find a
solution if one exists.  However, a complete algorithm must also complete and
report back that a solution does not exist if indeed that is the case.
\cite{lavalle06} A search algorithm can be either optimal or suboptimal.  It
is optimal if it returns the solution with the lowest cost-path (if there is a 
solution), otherwise, it is suboptimal.\cite{norvig} Time and space complexity
are exactly what they sound like.  Time complexity is generally a measure of 
how long the algorithm will take to complete given an arbitrary number of
inputs.  Space complexity is how much additional space the algorithm must
allocate (above and beyond the initial problem data) in order to successfully
complete the algorithm.  Obviously, the constraints under which we perform our
search will determine which of these criteria is the most important.

\section{Algorithms}
Now that we have a basic understanding of the terminology and attributes of a
search algorithm, we will discuss what are termed \textbf{uninformed} (also 
sometimes called \textbf{brute-force}) search algorithms.  These algorithms
are generally the easiest to implement because they assume no knowledge about
the search space or the search problem.  Generally, if a \textbf{heuristic}
function $h(n)$ can be found that can reasonably estimate the path cost
from $n$ then an informed search should be used.  However, sometimes such
a function either cannot be found, is impractical to generate, or does not 
exist.  In these cases, uninformed search must be used.\cite{norvig}

\subsection{Breadth-first search}
Previously we mentioned visualizing the search space as a tree.  One advantage
to this approach is that a tree has no loops.  So as long as you check for
repeated states in your algorithm, you can avoid infinite loops.  The idea of
breadth first search is to search one level of the search tree at a time,
exhausting the search at any given level prior to iterating to any element of
the next level.  We also discussed that data structure would determine the 
behavior of the algorithm.  It turns out, in order to implement a breadth-first
search we can use a FIFO data structure (queue).\cite{norvig} This is possible
by simply adding the children of a given node to a processing queue one at a
time, and then dequeuing the next item from the process queue and doing the
same with that item. The path to the current item must be stored (for the path
represents the potential solution) and redundant paths to any particular state
are ignored.\cite{norvig} A breadth-first search is a complete search, provided
the search space is finite.  It is trivial to prove that all nodes will be
explored with a breadth-first algorithm.  Further, breadth-first searches are
optimal given certain conditions -- namely that the path cost is a
non-descending function.  The most common scenario where this is true is where
the path cost is constant.\cite{norvig}  We will see this in the second part of
the paper when we deal with the graph search problem. The primary drawbacks to
breadth-first search are found in the area of complexity.  Assuming that each
parent has $b$ children (this is sometimes referred to as the 
\textbf{branching factor}), and the solution is at depth $d$ then the 
time complexity is:\vspace{5 mm}

$b + b^2 + ... + b^d = O(b^d)$ \cite{norvig}\vspace{5 mm}

It is easy to picture this since, on any given level, each parent node will
spawn $b$ children.  On a related note, the space complexity will be
determined by the size of the frontier.\cite{norvig} So the space complexity is
also $O(b^d)$.  This is a severe limitation on the
practical applications of uninformed breadth-first searching, as it will be
practically impossible to solve any but the smallest problems (assuming there
is no way to reduce the branching factor as the problem progresses).

\subsection{Uniform-cost search}
Uniform-cost search is closely related to breadth-first search.  The first
major difference between the two is that, while breadth-first search
expands the most \textit{shallow} node first, uniform-cost search will expand
the node with the lowest total \textit{path cost} first.\cite{norvig} This
requires a different ordering of the process queue.  As mentioned above,
breadth-first search is optimal only in those situations where the path-cost
function $g(n)$ is non-decreasing.  Uniform-cost search has the
advantage that it is \textit{always} optimal.  There are two other subtle
differences between breadth-first and uniform-cost search.  The first is that,
whereas the goal test in breadth-first search is applied before the node is
put in the process queue, in uniform cost search the goal test is applied
after the node is dequeued and is ready to be expanded.\cite{norvig} The reason
for this should be clear -- we want to find the best solution, and there may
be more efficient solutions that are children of nodes that have not yet been
expanded. The other difference is that, instead of ignoring paths to already
explored states, a check must be performed to determine which path is shorter,
and the node cost must be updated accordingly.\cite{norvig} Uniform-cost search
guarantees completeness provided no infinite sequence of zero cost actions is
encountered (i.e., the cost of each step is greater than some positive constant
$\epsilon$).\cite{norvig}  Given that the cost of the optimal solution is $C^*$,
the time and space complexity can be expressed as followed:\vspace{5 mm}

$O(b^{1 + \lfloor C^* / \epsilon \rfloor})$\cite{norvig}\vspace{5mm}

This is a direct result of the fact that uniform-cost search will explore those
nodes that have accumulated the least cost with no regard for the number of
steps that have been evaluated to that point.

\subsection{Depth-first search}
In contrast to the expansion policy of breadth-first search (i.e., shallowest
node first), depth-first search always expands the \textit{deepest} node first.
Generally, this can be implemented by replacing the FIFO queue in a 
breadth-first search with a LIFO stack.\cite{norvig}  Again, as long as the
state space is finite (and repeated states are avoided), depth-first search is
a complete algorithm as it will eventually span the entire search
space.\cite{norvig} However, depth-first search is non optimal since it returns
the first solution it encounters without any regard to whether a less
costly solution exists in the search space. As a consequence of this, the time
complexity of depth first search must be given in terms of $m$, the
maximum depth of any node.  With this in mind the time complexity of depth-first
search is $O(b^m)$, where $m$ can be much larger than the $d$ from
breadth-first search.\cite{norvig} Where depth-first search really shines is in
the space complexity category.  Because a stack is used as the data structure,
the maximum number of nodes in the stack for each level is simply the branching
factor $b$.  By its nature, the algorithm guarantees that all children
of one node are popped from the stack before any children of another node on
the same level are added to the stack.  Therefore, the space complexity of 
depth-first search is $O(bm)$.  It is possible to further cut down on this 
space requirement by only partially expanding nodes.  This type of approach is
termed \textbf{backtracking search}.\cite{norvig} 

\subsection{Depth-limited Search}
Depth limited search is exactly the same as depth-first search, but there is a
limit on the depth to which the algorithm will traverse the search tree prior
to giving up.  This is comes in handy when the search space is infinite in
nature.  Whereas traditional depth-first search can possibly fail in infinite
search spaces (more precisely it will continue forever), adding a maximum depth
alleviates this issue.\cite{norvig} We can ensure that this never happens by
introducing a limit $\ell$.  The interplay between $\ell$ and $d$ becomes
important because it affects the properties of the Depth-limited search.  For
instance, if $\ell < d$ then the search is incomplete (i.e., a solution exists
but the search will exhaust the available search space without finding that
solution). If $\ell > d$ then the search is non-optimal (i.e., it may return a
solution at a depth greater than $d$).\cite{norvig}  In the most unlikely of
circumstances where $\ell = d$, then the algorithm is obviously optimal.
Depth-limited search is useful when the path to a given solution has a known
maximum depth. This type of information can be use, for example, to solve a
Rubik's cube, since it has been mathematically shown that the maximum path to a
solution is 20, so we can set $\ell$ to 20 because we know that any Rubik's
cube can be solved in 20 moves or less.\cite{god20}

\subsection{Iterative deepening depth-first search}
Iterative deepening depth first search looks to take the depth-limited search
and ensure that it performs optimally. It accomplishes this task by dynamically
iterating the depth to which the depth-limited search will execute as the
search at any given depth is exhausted. At any given depth, if a goal state
is not found, then $\ell$ is incremented by one.  Presumably, at some point,
$\ell$ will equal $d$ and we will find our optimal solution.\cite{norvig} While
some time and space is wasted generating states from previous iterations, this
additional time and space is minuscule.  Because the majority of the nodes for
a given tree exist at the greatest depth, the relative cost to regenerate any
previous level decreases as the branching factor increases.\cite{norvig} For
many problems where the depth of the solution is not known and the search space
is incredibly large, iterative deepening search is a common choice for an
uninformed search algorithm.\cite{norvig}

\subsection{Bidirectional search}
One final uninformed search algorithm we will mention is the bidirectional
search.  The idea behind this search is to being one search with a initial
state and one search with the goal state and work back towards some middle
point.  The goal test of the traditional search algorithms is traded out for a
test that will determine whether the frontiers of the two searches intersect at
any node.  If there is an intersection then a path has been found.\cite{norvig}
The biggest benefit of this algorithm is that it cuts down the time complexity
of finding a solution from $O(b^d)$ to $O(b^{d / 2})$.  This is obviously an
incredible improvement over breadth-first search, but it comes at a price.  At
any given time, the frontiers of the two search algorithms must be kept in
memory so that intersection can be checked for.  This means space complexity of
$O(b^{d / 2})$ as well, which is quite substantial. There is also the more
practical problem of being able to generate predecessor states.  However, in
some problems, all transformations are reversible (e.g., solving a Rubik's
cube).  When this is the case, generating the predecessors is the same as
generating the successors.\cite{norvig} We will use a bidirectional search on
our problem in Chapter 2.

\section{Conclusion}
This concludes our overview of the characteristics of naive, uninformed
searching.  We have seen that, in general, a more sophisticated search
algorithm that relies on a heuristic function $h(n)$ is preferred.  However, in
those situations where such a function is impractical or impossible to
generate, various uninformed searches can be chosen based on the constraints of
the problem at hand.  Different algorithms have different strengths and
weaknesses, and we will be exploring some of those strengths and weaknesses in
Chapter 2 of this paper.

\chapter{Graph Search Experiment and Data}
Our original intention for this part of the paper was to create a Rubik's cube
solver that utilized different uninformed search strategies.  It turns out this
is a pretty terrible idea since the Rubik's cube search space has a branching
factor of 18. Accompanying the branching factor with the maximum depth of 20,
it is easy to see that getting past a depth of 6 or 7 becomes quite a laborious
task. As an exercise, if we are able to expand 1,000,000 nodes per second, it
would take somewhere on the order of magnitude of 400,000,000,000 years to
expand the entire search tree.  Of course, there are some tricks and what not
using symmetry to whittle that down a bit, but it is still an unmanageable
search tree.\newline

\section{Methodology}
Our problem consists of a search tree generated with a very low branching
factor of approximately .07 and a total of 10,000 search states. The script we
utilized to generate the test data has been included (along with all other
scripts we wrote for this portion of the paper) have been included along with
the submission of this paper.  We generated 50 files with different, random
graphs and utilized the above algorithms to find paths from node 0 to node
9999.  Ideally, we will see that the breadth-first search yields the optimal
solution, and each of the remaining algorithms will display characteristics in
accordance with Chapter 1.

\section{Technical Details}
As mentioned previously, we intended to create a Rubik's cube solver as our
project, but quickly realized that if we wanted to review the uninformed
searches this would be impossible (generally, Rubik's cube solvers rely on good
heuristic functions, and therefore must be solved with informed search).
Therefore, we invented a "toy" problem to illustrate the differences between
the uninformed search algorithms. We chose to use Python as our programming
language because it incorporates functional, imperative, and object-oriented
paradigms.  For the searches, generally, we represented the nodes with a very
basic class which had the node id number and a list of successors and
predecessors for each node.  The nodes were stored in a dictionary structure
indexed by the node id.  We built the graph dynamically from the data files we
generated with our generation script.  We decided to use trivial graph format
for the file as the string and list functions offered by Python make creating
the nodes quite trivial. Our test script executed the searches sequentially
on the 50 test cases and we condensed the results into some more useful forms.
One issue we ran into involved the depth-limited and iterative deepening
searches.  We realized during implementation that we did not need to keep
track of the visited nodes because we only cared about the nodes in the current
path.  This lead to some more concise code in those two algorithms, though
it also meant we had to convert a list to a set for each path popped from
the stack.

\section{The Results}
Below is a table summarizing the relative average path lengths of the 50
trials as well as the average execution time:\vspace{5 mm}

\begin{tabular}{ | l | r | r | }
  \hline
  Algorithm & Avg. Cost & Avg. Time (s) \\ \hline
  Breadth-First & 6.04 & 0.02462976 \\
  Depth-First & 1451.24 & 0.37136162 \\
  Depth-Limited & 6.84 & 0.01619128 \\
  Iterative Deepening & 6.04 & 0.02395116 \\
  Bidirectional & 6.96 & 0.01949264 \\
  \hline
\end{tabular}
\vspace{5 mm}

As an aside, for these searches we assume that the path cost is constant at 1
unit per transition, so there was no need for us to implement a uniform cost
search.  Somewhat surprisingly, these results are generally what one would
expect. For instance, we expect the breadth-first search to have the lowest
cost for every graph, and therefore the lowest average cost.  This is supported
by the data in the table, as well as the raw data. We also expect that the
iterative deepening search will have the exact same cost as the breadth-first
search in every instance.  This is also supported by the data.  Both
breadth-first search and iterative deepening search are optimal, so they should
always generate the "best" solution (i.e., the lowest path cost). Below is a
sampling of the raw data from both the breadth-first and iterative deepening
searches.  While the paths generated were not always identical (not shown), the
length of those paths always corresponded.\vspace{5 mm}

\begin{tabular}{ | l | r | r | l | }
  \hline
  Iteration & Breadth First & Depth First & Equal \\ \hline
  0 & 6 & 6 & TRUE \\
  1 & 7 & 6 & TRUE \\
  2 & 7 & 7 & TRUE \\
  3 & 6 & 6 & TRUE \\
  4 & 7 & 7 & TRUE \\
  5 & 5 & 5 & TRUE \\
  6 & 7 & 7 & TRUE \\
  7 & 6 & 6 & TRUE \\
  8 & 6 & 6 & TRUE \\
  9 & 6 & 6 & TRUE \\
  \hline
\end{tabular}
\vspace{5 mm}

The above pattern continues for all 50 test cases. Similarly, if we compare the
execution times of the two algorithms they are nearly identical, which is 
evidence of what we said earlier about re-expanding nodes from a previous
level.  Indeed, our branching factor is less than 1, so it's not surprising
that the iterative deepening search performed slightly \textit{better} than the
breadth-first search.\newline
\indent The next interesting thing to note is the average cost of the
depth-first search is enormous compared to the other searches.  This is to be
expected with the properties of our graph.  There are many many nodes, and not
very many edges, so some quite lengthy paths are bound to exist.  Further,
since depth-first search returns the \textit{first} successful path, it is most
likely not going to be the optimal path.  Obviously $m$ for any given graph is
likely to be quite large given the parameters with which we set up our search
space.  This is reflected in the relatively long paths generated as well as the
long execution times.\newline 
\indent For our depth-limited search we set the depth limit to 7.  Somewhat
coincidentally and a bit unluckily, only one optimal path exceeded length 7
(i.e., $\ell < d$ in trial 31), so we only illustrated the "incomplete"
attribute of a depth-limited search on one occasion. However, many times when
the optimal path was 5 or 6 the depth-limited search would generate a solution
of length 7.  So this illustrates that depth-limited search is not optimal when
$\ell > d$. We can see that depth-limited search is optimal when $\ell = d$ by
looking at the iterative deepening search, which we discussed
previously.\newline 
\indent The bidirectional search also behaved as expected with the exception
that it executed slightly more slowly that the iterative deepening search. We
expected it to outperform all the searches with an $O(b^{d / 2})$ time
complexity.  However, it did not outperform the iterative deepening.  We chalk
this up to inefficient use of data structures (e.g., several conversions from
lists to sets that may have been avoided by a more skilled programmer).
However, that being said, it did outperform the breadth first search by a
decent margin.

\begin{thebibliography}{9}

  \bibitem{norvig}
    Russel, Stuart and Peter Norvig,
    \textit{Artificial Intelligence: A Modern Approach}.
    Prentice Hall, Upper Saddle River,
    3rd Edition,
    2010. Print.

  \bibitem{wikiAgent}
    Intelligent Agent. 21 April, 2013.  
    In \textit{Wikipedia}.  Retrieved April 25, 2013,
    from http://en.wikipedia.org/wiki/intelligent\_agent
  
  \bibitem{wikiSpace}
    Search Space. 17 March, 2013.  
    In \textit{Wikipedia}.  Retrieved April 25, 2013,
    from http://en.wikipedia.org/wiki/search\_space
  
  \bibitem{shun}
    Long, Shun, Hui-Jin Wang, Jian-Hua Cai, and Chan-Juan Liu. 
    "Enhancing Intelligent Agents with Information Retrieval Techniques." 
    \textit{Fourth International Conference on Natural Computation} 2 
    (2008): 627-31. Print.

  \bibitem{zhang99}
    Zhang, Weixiong. \textit{State-space Search: Algorithms, Complexity,
    Extensions, and Applications}. New York: Springer, 1999. Print.

  \bibitem{lavalle06}
    LaValle, Steven M., \textit{Planning Algorithms}.
    Cambridge University Press. 2006. Web.
    Retrieved April 26, 2013, from http://planning.cs.uiuc.edu/node631.html
 
  \bibitem{god20}
    Fildes, Jonathan. "Rubik's Cube Quest for Speedy Solution Comes to an End."
    \textit{BBC News}. BBC, 08 Nov. 2010. Web. 27 Apr. 2013.

\end{thebibliography}

\end{document}

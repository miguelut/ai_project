\documentclass[a4paper,11pt]{report}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[margin=1.0in]{geometry}
\usepackage{setspace}
\onehalfspacing

\title{AI: An Overview of Uninformed Searching and Applications}
\author{Dylan Thompson \and Michael M. Wright}

\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
This paper begins with a broad overview of uninformed search algorithms in the 
field of Artificial Intelligence.  While search is useful for a variety of toy 
problems such as the Wolf-Goat-Cabbage problem and the Missionaries and
Cannibals problem, this paper will discuss some of the real-world problems
for which uninformed searching is an acceptable solution.  The paper will then 
analyze some of the different search algorithms as they relate to solving a 
Rubik's cube.

\textbf{Don't forget to add your conclusions!}

\end{abstract}

\chapter{Uninformed Searching}
A small portion of solvable problems can be modeled in computers simply by
mapping the entire set of conditions to corresponding actions.  However, for
some problems this mapping is impractical if not impossible given the
constraints of our current technology.  In situations like these it becomes
imperative to build a \textbf{goal-based} agent.  Goal-based agents can, in
effect, consider future actions and evaluate the effect of those future
actions on reaching the desired solution.\cite{norvig}  The two main
subfields of goal-based agents are search and planning.\cite{wikiAgent} The
field of searching can be further whittled down into those algorithms that
utilize uninformed algorithms and those that utilize informed algorithms.
Later in the chapter we will look at the uninformed searching algorithms in
use today.  First, however, we must consider the criteria a problem must meet
before we can use a search agent to solve that problem.  It turns out that a
problem needs to be \textbf{well-defined} (or well-structured) prior to 
employing a searching agent to solve the problem.\cite{shun}

\section{Defining the Problem}
If we think about it there are some fairly obvious things we need to know
before we attempt to solve a problem.  For instance, we definitely need to be
able to check if we've actually solved it.  Also, we will presumably need a
place to start, or an initial state. Further, we need a set of actions our 
agent can take along with transition model that will give us a new state after
we have taken an action.  The final, less obvious thing we need is a path cost.
\cite{norvig}  More concisely, a well-defined problem consists of these
five items:

\begin{itemize}
\item An \textbf{initial state}.
\item A set of \textbf{actions}.
\item A \textbf{transition model}.
\item A \textbf{goal-test}.
\item A \textbf{path cost}.\cite{norvig}
\end{itemize}

Once the problem is defined, we can go about building our agent to search for
a solution.  However, as we shall see in the next sections, not all solutions
were created equal.

\section{Search Properties}
In much of the literature regarding searching many authors refer to the
\textbf{search space}.  Somewhat informally, the search space is the solution
space defined by the set of all possible candidate solutions.\cite{wikiSpace}
More formally, the concept of \textbf{state space}, a synonym of search space,
has been defined as a 4-tuple containing the following elements:

\begin{itemize}
\item A set of states N.
\item A set of arcs A.
\item A nonempty subset of N, S, that contains start states.
\item A nonempty subset of N, G, that contains goal states.\cite{zhang99}
\end{itemize}

For goal-based agents it is generally easiest to picture the search space as a
tree, though it is more precisely a directed graph.\cite{zhang99}  We build the
tree by expanding the nodes.  Each node of the tree is a state, and the
connections between the nodes are abstractions of the relationship between
actions and the transition function.  In other words, each action will yield a
child node, and thus the tree expands for each action explored.  When we talk
about search we are really talking about different methods for expanding the
tree.\newline
\indent At any point in time prior to reaching a solution to our problem we
have a set of "temporary" leaf nodes (assuming at least one action is available
for the nodes) that can be expanded.  This is called the \textbf{frontier}
or open list.\cite{norvig}  The manner in which we choose which frontier node
to expand will be the defining characteristic of the searches we talk about
in the following sections.  Do we want to expand all the nodes at the same
depth first?  Or perhaps we want to follow a single path as deep as it will go
before moving on to the next path.  In addition, do we need to check for loops
in our path, or will the nature of our algorithm make checking for repeated
states a needless overhead?  Determining which state to expand next is generally
called the \textbf{search strategy}.\cite{norvig}\newline
\indent Implementing a search strategy requires some specific state that must
be tracked through each iteration of the algorithm. As mentioned before, the
entire state space is generally too large to keep in memory, and therefore the
state space is generally implicit in the search.\cite{norvig} At a minimum, the
search algorithm data structure needs the following four items:

\begin{itemize}
\item The \textbf{state}.
\item The node's \textbf{parent}.
\item The \textbf{action} that was applied to the parent.
\item The \textbf{path cost}.\cite{norvig}
\end{itemize}

Together with the elements of a well-defined problem it is easy to see how to
generate a child node given a parent node and an action.  Given a state and an
action we can apply the transition model to get the child node's state.  The
path cost is then computed with the information from the problem, and can be
constant or dynamic in nature.  Generally, choosing the data structure used in 
the search will have a huge determination on the search strategy.  A queue 
is FIFO and will have a very different search outcome from a LIFO stack or a 
priority queue.\cite{norvig}\newline

\section{Search Performance}
When searching we often find that some attributes of the search are more
important to us than others.  For instance, sometimes time may be of
paramount importance, while other times our search may operate under certain
memory constraints that make conservation of space take priority.  The various
algorithms we will consider in the next few sections will each have strengths
and weaknesses in the following categories:

\begin{itemize}
\item \textbf{Completeness}.
\item \textbf{Optimality}.
\item \textbf{Time Complexity}.
\item \textbf{Space Complexity}.
\end{itemize}

Completeness is a measure of whether an algorithm is guaranteed to find a
solution if one exists.  However, a complete algorithm must also complete and
report back that a solution does not exist if indeed that is the case.
\cite{lavalle06} A search algorithm can be either optimal or suboptimal.  It
is optimal if it returns the solution with the lowest cost-path (if there is a 
solution), otherwise, it is suboptimal.\cite{norvig} Time and space complexity
are exactly what they sound like.  Time complexity is generally a measure of 
how long the algorithm will take to complete given an arbitrary number of
inputs.  Space complexity is how much additional space the algorithm must
allocate (above and beyond the initial problem data) in order to successfully
complete the algorithm.  Obviously, the constraints under which we perform our
search will determine which of these criteria is the most important.

\section{Search Algorithms}
Now that we have a basic understanding of the terminology and attributes of a
search algorithm, we will discuss what are termed \textbf{uninformed} (also 
sometimes called \textbf{brute-force}) search algorithms.  These algorithms
are generally the easiest to implement because they assume no knowledge about
the search space or the search problem.  Generally, if a \textbf{heuristic}
function \textit{h(n)} can be found that can reasonably estimate the path cost
from \textit{n} then an informed search should be used.  However, sometimes such
a function either cannot be found, is impractical to generate, or does not 
exist.  In these cases, uninformed search must be used.\cite{norvig}

\subsection{Breadth-first search}
Previously we mentioned visualizing the search space as a tree.  The idea of
breadth first search is to search one level of the search tree at a time,
exhausting the search at any given level prior to iterating to any element of
the next level.  We also discussed that data structure would determine the 
behavior of the algorithm.  It turns out, in order to implement a breadth-first
search we can use a FIFO data structure (queue).\cite{norvig} This is possible
by simply adding the children of a given node to a processing queue one at a
time, and then dequeuing the next item from the process queue and doing the
same with that item. The path to the current item must be store (for the path
represents the potential solution) and redundant paths to any particular state
are ignored.\cite{norvig} A breadth-first search is a complete search, provided
the search space is finite.  It is trivial to prove that all nodes will be
explored with a breadth-first algorithm.  Further, breadth-first searches are
optimal given certain conditions -- namely that the path cost is a
non-descending function.  The most common scenario where this is true is where
the path cost is constant.\cite{norvig}  We will see this in the second part of
the paper when we deal with the Rubik's cube problem. The primary drawbacks to
breadth-first search are found in the area of complexity.  Assuming that each
parent has \textit{b} children (this is sometimes referred to as the 
\textbf{branching factor}), and the solution is at depth \textit{d} then the 
time complexity is:\vspace{5 mm}

\begin{math}
  b + b^2 + ... + b^d = O(b^d) \cite{norvig}
\end{math}

\vspace{5 mm}

It is easy to picture this since, on any given level, each parent node will
spawn \textit{b} children.  On a related note, the space complexity will be
determined by the size of the frontier.\cite{norvig} So the space complexity is
also \begin{math} O(b^d) \end{math}.  This is a severe limitation on the
practical applications of uninformed breadth-first searching, as it will be
practically impossible to solve any but the smallest problems (assuming there
is no way to reduce the branching factor as the problem progresses).

\begin{thebibliography}{9}

  \bibitem{norvig}
    Russel, Stuart and Peter Norvig,
    \textit{Artificial Intelligence: A Modern Approach}.
    Prentice Hall, Upper Saddle River,
    3rd Edition,
    2010. Print.

  \bibitem{wikiAgent}
    Intelligent Agent. 21 April, 2013.  
    In \textit{Wikipedia}.  Retrieved April 25, 2013,
    from http://en.wikipedia.org/wiki/intelligent\_agent
  
  \bibitem{wikiSpace}
    Search Space. 17 March, 2013.  
    In \textit{Wikipedia}.  Retrieved April 25, 2013,
    from http://en.wikipedia.org/wiki/search\_space
  
  \bibitem{shun}
    Long, Shun, Hui-Jin Wang, Jian-Hua Cai, and Chan-Juan Liu. 
    "Enhancing Intelligent Agents with Information Retrieval Techniques." 
    \textit{Fourth International Conference on Natural Computation} 2 
    (2008): 627-31. Print.

  \bibitem{zhang99}
    Zhang, Weixiong. \textit{State-space Search: Algorithms, Complexity,
    Extensions, and Applications}. New York: Springer, 1999. Print.

  \bibitem{lavalle06}
    LaValle, Steven M., \textit{Planning Algorithms}.
    Cambridge University Press. 2006. Web.
    Retrieved from http://planning.cs.uiuc.edu/node631.html
 

\end{thebibliography}

\end{document}
